## What is big data 
Big data refers to a collection of data that is not only massive in size but also expanding at a fast pace over time. Because of its massive quantity and high complexity, none of the standard techniques for managing data can store or handle it in an effective manner. Big data is simply data that is stored in extremely large quantities. On the other hand, these enormous amounts of data, when properly analyses with modern instruments, offer organisations valuable insights that enable them to make more effective business choices.
### Examples of Big data
#### Big Data is in healthcare
When it comes to big data in healthcare, we can observe that it is extensively used. It involves collecting data, analysing it, and utilising it for customers. Also, the clinical data of patients is too complicated for conventional techniques to handle or understand. Because big data is handled by machine learning algorithms and data scientists, dealing with such massive amounts of data becomes manageable. Doctors nowadays rely heavily on patients' clinical records, which means that a large amount of data must be acquired for a variety of patients. This data cannot be stored using outdated or standard data storage methods. Because there is a large amount of data flowing from numerous sources and in various forms, the necessity to handle this large amount of data expands, which is why the Big Data method is required.
### Types of big data
Big Data is divided into three categories:
#### Structured Data
Structured data refers to any data that can be processed, is easily available, and can be kept in a consistent format. Structured data is the easiest to work with in Big Data since it contains carefully coordinated measurements that are established by defining parameters.
#### Unstructured Data
In the context of Big Data, the term "unstructured data" refers to situations in which the data format consists of many unstructured files which include audio recordings, videos. This type of data is referred to as complicated data due to the fact that its structure is not very recognisable and it is a large amount. An output that is returned by a search engine such as Google Search or Yahoo Search is a glaring example of unstructured data.
#### Semi-structured Data
Semi-structured data is a combination of unstructured and structured data in Big Data. This data type has the characteristics of structured data, but it also contains unstructured information that does not comply to any formal structure of data models or any relational database.
###  6 ‘V’s of Big Data
The general properties of Big Data are referred as the following:
#### Volume: 
The volume of a dataset being processed and saved in the Big Data System is known as its most important and conspicuous attribute. Data sizes often range from petabytes to exabytes, and it is processed using modern processing technology. 
#### Velocity:
Velocity is also known as the data collection rate, and it helps analysts assess whether the data is classified as regular or Big Data. Data demands real-time examination, which necessitates well-integrated systems to handle the volume and rate of created data.  
#### Variety:
Variety can be defined as the type of data structure as well as how it is organised and prepared for processing. The rate at which data collects effects if the data is categorised as large-scale information or ordinary data. The speed of processing data generally indicates that more information will be available than in the previous batch, as well as a high data processing rate. 
#### Veracity:
It is the quality and dependability of the data in question. Untrustworthy data undermines the credibility of Big Data, especially when it is updated in real time. As a result, data authenticity necessitates constant checks at every stage of collection and processing.  
#### Value:
value should also be considered when gathering and analysing Big Data. More than the quantity of data, the value of that data is critical for gaining insights. 
#### Variability:
Variability is a Big Data feature that allows it to be formatted and used for actionable purposes.
### Phases of Big Data analysis
#### Data discovery:
The data science team conducts problem investigation, develops context, understands data sources, formulates initial hypotheses, and tests them with data.
#### Data Preparation:
Data preparation involves exploring, preprocessing, and conditioning data before modelling and analysis, requiring an analytic sandbox and tools like Hadoop, Alpine Miner, and Open Refine.
#### Model Planning: 
The team looks at the data to find out how the variables are related to each other. They then choose the most important variables and the best models. This is the part where the data science team creates sets of data for training, testing, and production. Based on the work done in the model planning step, the team builds and runs the models. A lot of the time, Matlab and STASTICA are used for this step.
#### Model Building:
The team creates models that can be used for training, testing, and production. They also think about whether the tools they already have will be enough to run the models or if they need a more stable setting to do so. Here are some free or open-source tools: Octave, WEKA, and Rand PL/R
#### Communication Results:
Once the model has been run, the team must compare the results to the standards they set for success and failure. The team thinks about how to best explain results and outcomes to different team members and clients while keeping warnings and assumptions in mind. The team should figure out what the most important findings are, how much they are worth to the business, and how to summarise and share those findings with users.
#### Operationalize: 
The team tells more people about the project's benefits and sets up a sample project to test the work in a safe environment before releasing it to the whole company. With this method, the team can test the model's performance and any limitations in a production setting on a small scale, then make changes before deploying the whole thing. The team gives out final codes, explanations, and reports. Octave, WEKA, SQL, and MADlib are all free or open-source tools.
### Challenges in Big Data analysis
#### Heterogeneity and Incompleteness:
Machine analysis algorithms have difficulty understanding complicated data, which necessitates additional work for effective representation, access, and analysis of semi-structured data. During probabilistic analysis, probable incompleteness and errors require control due to the fact that they are necessary.
#### Scale:
The increasing volume of data is taking up computers, but cloud computing offers the possibility of a solution to this problem. Developments in storage units, such as the shift from hard disc drives to solid state drives and the introduction of parallel file systems, could have an effect on the processing of data and the efficiency with which it is done. It is anticipated that this problem would continue to worsen exponentially.
#### Timeliness:
When working with a big data set, it is frequently required to locate components within the set that satisfy particular requirements. 
Frequently, the issue is in the queries or the searches.On the basis of the characteristics of the dataset, new index structures are required.
#### privacy:
People are increasingly concerned about the protection of their data when it comes to Big Data.
Big data can assist us in figuring out new things by analysing data for factors such as location, time, and a variety of other qualities.
The protection of personal information is an issue that must be addressed from both a social and a technical perspective.
#### Human Collaboration:
In order to conduct an accurate analysis of large quantities of data, it is essential to combine the use of machine learning with human analysis.
Users of an analysis system for Big Data need to have access to the findings of the analysis, and the system itself needs to be flexible enough to accommodate the input of a diverse group of human specialists, each of whom is an authority in their own field. 
### References
Taylor, D. (2023). What is Big Data? Introduction, Types, Characteristics, Examples. Guru99. https://www.guru99.com/what-is-big-data.html

What is Big Data: Types, Characteristics and Benefits. (n.d.). https://www.knowledgehut.com/blog/big-data/types-of-big-data

Fan, J., Han, F., & Liu, H. (2014). Challenges of Big Data analysis. National Science Review, 1(2), 293–314. https://doi.org/10.1093/nsr/nwt032

GeeksforGeeks. (2019). Big Challenges with Big Data. GeeksforGeeks. https://www.geeksforgeeks.org/big-challenges-with-big-data/


